# ğŸ•·ï¸ Servimed Scraper - Sistema Completo com Scrapy

Sistema completo de scraping do portal Servimed com trÃªs nÃ­veis de execuÃ§Ã£o, sempre utilizando **Scrapy** como framework principal.

## ğŸ“‹ VisÃ£o Geral

- **NÃ­vel 1**: ExecuÃ§Ã£o direta (sÃ­ncrona)
- **NÃ­vel 2**: Sistema de filas com Celery (assÃ­ncrona)
- **NÃ­vel 3**: Sistema completo de pedidos

## ğŸš€ InstalaÃ§Ã£o

### 1. DependÃªncias
```bash
pip install -r requirements.txt
```

### 2. ConfiguraÃ§Ã£o
Copie o arquivo `.env.example` para `.env` e configure suas credenciais:
```bash
cp .env.example .env
```

Edite o arquivo `.env` com suas credenciais do Servimed.

### 3. Redis (para NÃ­veis 2 e 3)
```bash
# Windows - usando Redis portable
redis_start.bat

# Linux/Mac
redis-server
```

## ğŸ¯ Como Usar

### **NÃ­vel 1 - ExecuÃ§Ã£o Direta**
```bash
# Executar scraping direto
python main.py --nivel 1

# Com filtro
python main.py --nivel 1 --filtro "paracetamol"

# Limitando pÃ¡ginas
python main.py --nivel 1 --filtro "vitamina" --max-pages 5
```

### **NÃ­vel 2 - Sistema de Filas**
```bash
# 1. Iniciar worker Celery
start_worker.bat

# 2. Enfileirar tarefa
python main.py --nivel 2 --enqueue --filtro "dipirona"

# 3. Verificar status
python main.py --nivel 2 --status <task_id>
```

### **NÃ­vel 3 - Sistema de Pedidos**
```bash
# 1. Iniciar worker Celery
start_worker.bat

# 2. Teste de pedido
python pedido_queue_client.py test

# 3. Pedido personalizado
python pedido_queue_client.py enqueue PEDIDO123 444212 2

# 4. Verificar status
python pedido_queue_client.py status <task_id>
```

## ğŸ“ Estrutura do Projeto

```
â”œâ”€â”€ main.py                     # Script principal
â”œâ”€â”€ pedido_queue_client.py      # Cliente para pedidos (NÃ­vel 3)
â”œâ”€â”€ requirements.txt            # DependÃªncias Python
â”œâ”€â”€ .env                       # ConfiguraÃ§Ãµes (criar a partir do .example)
â”œâ”€â”€ start_worker.bat           # Script para iniciar worker Celery
â”œâ”€â”€ redis_start.bat            # Script para iniciar Redis
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scrapy_servimed/       # Projeto Scrapy
â”‚   â”œâ”€â”€ scrapy_wrapper.py      # Wrapper do Scrapy
â”‚   â”œâ”€â”€ servimed_scraper/      # Framework original (fallback)
â”‚   â”œâ”€â”€ nivel2/               # Sistema de filas
â”‚   â”‚   â”œâ”€â”€ celery_app.py     # ConfiguraÃ§Ã£o Celery
â”‚   â”‚   â”œâ”€â”€ tasks.py          # Tarefas assÃ­ncronas
â”‚   â”‚   â””â”€â”€ queue_client.py   # Cliente de filas
â”‚   â”œâ”€â”€ nivel3/               # Sistema de pedidos
â”‚   â”‚   â”œâ”€â”€ tasks.py          # Tarefas de pedidos
â”‚   â”‚   â””â”€â”€ pedido_client.py  # Cliente de pedidos
â”‚   â””â”€â”€ api_client/           # Cliente para APIs externas
â””â”€â”€ data/                     # Arquivos de saÃ­da
```

## ğŸ•·ï¸ Framework Scrapy

O sistema **sempre usa Scrapy** como framework principal:

- âœ… **Performance superior**
- âœ… **Melhor gestÃ£o de recursos**
- âœ… **Logs detalhados**
- âœ… **Fallback automÃ¡tico** para framework original em caso de erro

## ğŸ”§ ConfiguraÃ§Ãµes Importantes

### Credenciais (.env)
```env
# Portal Servimed
PORTAL_EMAIL=seu_email@empresa.com.br
PORTAL_PASSWORD=sua_senha

# API Callback
CALLBACK_API_USER=seu_email@empresa.com.br
CALLBACK_API_PASSWORD=sua_senha
CALLBACK_URL=https://desafio.cotefacil.net
```

### Redis
- **Porta**: 6379 (padrÃ£o)
- **Database**: 0
- **NecessÃ¡rio para**: NÃ­veis 2 e 3

### Celery
- **Broker**: Redis
- **Pool**: solo (compatÃ­vel com Windows)
- **Workers**: Iniciar via `start_worker.bat`

## ğŸ“Š Arquivos de SaÃ­da

- `data/servimed_produtos_scrapy.json` - Produtos coletados pelo Scrapy
- `data/servimed_backup.json` - Backup automÃ¡tico
- Logs detalhados no console

## ğŸ› ï¸ SoluÃ§Ã£o de Problemas

### Erro de Import
```bash
# Adicionar src ao PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:./src"  # Linux/Mac
$env:PYTHONPATH += ";.\src"              # Windows PowerShell
```

### Redis nÃ£o conecta
```bash
# Verificar se Redis estÃ¡ rodando
redis-cli ping
```

### Worker Celery nÃ£o inicia
```bash
# Matar processos Python antigos
taskkill /f /im python.exe  # Windows
```

## ğŸ“ˆ Performance

- **Scrapy**: ~2-3 segundos por pÃ¡gina
- **Coleta**: 10-50 produtos por pÃ¡gina
- **Memory**: ~100MB em uso tÃ­pico
- **ConcorrÃªncia**: Suporte a mÃºltiplos workers

## ğŸ” SeguranÃ§a

- âœ… Credenciais em arquivo `.env` (nÃ£o versionado)
- âœ… AutenticaÃ§Ã£o OAuth2 para APIs
- âœ… Tokens com expiraÃ§Ã£o automÃ¡tica
- âœ… Logs sem dados sensÃ­veis

## ğŸ“ Logs

O sistema gera logs detalhados mostrando:
- Framework utilizado (sempre Scrapy)
- Produtos coletados
- Tempo de execuÃ§Ã£o
- Status das tarefas
- Erros e fallbacks

---

**ğŸ•·ï¸ Sistema 100% Scrapy - Framework moderno e eficiente! ğŸ•·ï¸**
