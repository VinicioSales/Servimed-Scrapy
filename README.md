# üìñ SERVIMED SCRAPER - DOCUMENTA√á√ÉO COMPLETA

## üèóÔ∏è VIS√ÉO GERAL

O **Servimed Scraper** √© um sistema completo de web scraping desenvolvido em Python para extrair informa√ß√µes de produtos do site Servimed. O projeto implementa uma arquitetura em 3 n√≠veis de complexidade, sempre utilizando o framework **Scrapy** como base.

### üéØ OBJETIVOS
- ‚úÖ Extrair dados de produtos farmac√™uticos do site Servimed
- ‚úÖ Oferecer m√∫ltiplas formas de execu√ß√£o (s√≠ncrona e ass√≠ncrona)
- ‚úÖ Implementar sistema de filas para processamento em larga escala
- ‚úÖ Fornecer API para integra√ß√£o com sistemas externos
- ‚úÖ Garantir qualidade atrav√©s de testes automatizados

### üìä STATUS DO PROJETO
- **Data:** 13 de Agosto de 2025
- **Status:** ‚úÖ **COMPLETO E FUNCIONAL**
- **Framework:** Scrapy 2.13.3
- **Testes:** ‚úÖ Sistema de testes automatizados implementado
- **Arquitetura:** 3 N√≠veis de Complexidade

---

## üèõÔ∏è ARQUITETURA DO SISTEMA

### üìä Estrutura de Diret√≥rios
```
PROVA/
‚îú‚îÄ‚îÄ üìÑ main.py                      # Arquivo principal - ponto de entrada
‚îú‚îÄ‚îÄ üìÑ requirements.txt             # Depend√™ncias do projeto
‚îú‚îÄ‚îÄ üìÑ .env                         # Vari√°veis de ambiente (configurar)
‚îú‚îÄ‚îÄ üìÑ pyproject.toml               # Configura√ß√£o do projeto (pytest, coverage)
‚îú‚îÄ‚îÄ üìÑ scrapy.cfg                   # Configura√ß√£o do Scrapy
‚îú‚îÄ‚îÄ üìÑ run_tests.py                 # Script para executar testes
‚îú‚îÄ‚îÄ üìÑ README.md                    # Esta documenta√ß√£o
‚îú‚îÄ‚îÄ 
‚îú‚îÄ‚îÄ üìÅ src/                        # C√≥digo fonte principal
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ scrapy_wrapper.py       # Wrapper principal do Scrapy
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ pedido_queue_client.py  # Cliente para pedidos (N√≠vel 3)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ config/                 # Configura√ß√µes do sistema
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ settings.py         # Configura√ß√µes internas da aplica√ß√£o
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ paths.py            # Caminhos e diret√≥rios do projeto
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ nivel2/                 # Sistema de filas (N√≠vel 2)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ queue_client.py     # Cliente de filas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ tasks.py            # Tarefas Celery
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ celery_app.py       # Configura√ß√£o Celery
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ scrapy_servimed/        # Projeto Scrapy
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ settings.py         # Configura√ß√µes Scrapy
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ pipelines.py        # Pipelines de processamento
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ items.py            # Defini√ß√£o de items
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ spiders/            # Spiders de scraping
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ üìÑ servimed_spider.py  # Spider principal
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ servimed_scraper/       # Sistema de processamento
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ celery_app.py       # App Celery principal
‚îÇ       ‚îî‚îÄ‚îÄ üìÑ tasks.py            # Tarefas de processamento
‚îÇ
‚îú‚îÄ‚îÄ üìÅ tests/                      # Testes automatizados
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ conftest.py             # Configura√ß√µes globais de teste
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ test_basic_functionality.py  # Testes b√°sicos ‚úÖ
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ test_config.py          # Testes de configura√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ test_main.py            # Testes do main.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ test_scrapy_wrapper.py  # Testes do wrapper
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ test_spiders.py         # Testes dos spiders
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ test_integration.py     # Testes de integra√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ test_nivel2/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ test_tasks.py       # Testes Celery
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ test_nivel3/
‚îÇ       ‚îî‚îÄ‚îÄ üìÑ test_pedido_queue_client.py  # Testes pedidos
‚îÇ
‚îú‚îÄ‚îÄ üìÅ data/                       # Dados e resultados
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ servimed_produtos_scrapy.json   # Resultados Scrapy
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ logs/                   # Logs do sistema

```

---

## üöÄ IN√çCIO R√ÅPIDO

### üìã Pr√©-requisitos
- ‚úÖ Python 3.10+
- ‚úÖ Conex√£o com Internet

### üîß Setup R√°pido

#### 1. Verificar se est√° tudo funcionando:
```bash
# Navegar para o diret√≥rio do projeto
cd servimed-scraper

# Testar ambiente b√°sico
python -m pytest tests/test_basic_functionality.py::TestBasicFunctionality::test_python_environment -v
```

#### 2. Executar primeiro teste do sistema:
```bash
# N√≠vel 1 - Execu√ß√£o direta (mais simples)
python main.py --nivel 1 --filtro "paracetamol" --max-pages 1
```

#### 3. Verificar resultados:
```bash
# Ver arquivo gerado
type data\servimed_produtos_scrapy.json
```

### üìù Exemplos Pr√°ticos

#### üéØ Exemplo 1: Busca Simples
```bash
# Buscar produtos com "dipirona" limitado a 2 p√°ginas
python main.py --nivel 1 --filtro "dipirona" --max-pages 2

# Resultado esperado: Arquivo JSON com produtos encontrados
```

#### üéØ Exemplo 2: Executar Testes
```bash
# Testes b√°sicos (sempre funcionam)
python -m pytest tests/test_basic_functionality.py -v
```

#### üéØ Exemplo 3: Script Interativo de Testes
```bash
# Executar menu interativo
python run_tests.py

# Escolher op√ß√£o 1: Executar todos os testes
# Escolher op√ß√£o 4: Verificar estrutura
```

---

## üéØ N√çVEIS DE EXECU√á√ÉO

### üéØ N√çVEL 1: EXECU√á√ÉO DIRETA (S√çNCRONA)
**Descri√ß√£o:** Execu√ß√£o direta e imediata usando Scrapy  

#### üîß Como Usar:
```bash
# Execu√ß√£o b√°sica
python main.py --nivel 1

# Com filtro de produtos
python main.py --nivel 1 --filtro "paracetamol"

# Limitando p√°ginas
python main.py --nivel 1 --max-pages 5

# Combinando op√ß√µes
python main.py --nivel 1 --filtro "dipirona" --max-pages 3
```

#### üìä Sa√≠da:
- Arquivo: `data/servimed_produtos_scrapy.json`
- Format: JSON com lista de produtos
- Logs: Console em tempo real

---

### üéØ N√çVEL 2: SISTEMA DE FILAS (ASS√çNCRONA)
**Descri√ß√£o:** Processamento via filas usando Celery + Redis  
**Uso:** Para processamento em larga escala e produ√ß√£o  
**Framework:** Scrapy + Celery + Redis

#### üîß Pr√©-requisitos:
```bash
# 1. Instalar e iniciar Redis
# Windows: Baixar Redis ou usar Docker
docker run -d -p 6379:6379 redis:latest

# 2. Iniciar worker Celery
python -m celery -A src.nivel2.celery_app worker --loglevel=info
```

#### üîß Como Usar:
```bash
# Verificar status dos workers
python main.py --nivel 2 --worker-status

# Enfileirar nova tarefa
python main.py --nivel 2 --enqueue --usuario "user@example.com" --senha "password"

# Verificar status de tarefa
python main.py --nivel 2 --status "task-id-aqui"

# Com filtro
python main.py --nivel 2 --enqueue --filtro "antibiotico"
```

#### üìä Sa√≠da:
- Task ID para acompanhamento
- Callback HTTP para notifica√ß√£o
- Resultados via API status

---

### üéØ N√çVEL 3: SISTEMA DE PEDIDOS
**Descri√ß√£o:** Sistema completo de processamento de pedidos  
**Framework:** Scrapy + Celery + Sistema de Pedidos

#### üîß Como Usar:
```bash
# Teste do sistema
python src/pedido_queue_client.py test

# Enfileirar pedido
python src/pedido_queue_client.py enqueue "PEDIDO123" "444212" "2" "1234567890123"

# Verificar status do pedido
python src/pedido_queue_client.py status "task-id"

# Via main.py (orienta√ß√µes)
python main.py --nivel 3
```

#### üìä Sa√≠da:
- Task ID do pedido
- Status detalhado via API
- Integra√ß√£o com sistemas externos

---

## ‚öôÔ∏è CONFIGURA√á√ÉO DO AMBIENTE

### üìã Depend√™ncias Principais
```txt
# === CORE (obrigat√≥rias) ===
scrapy>=2.11.0              # Framework de scraping principal
twisted>=22.10.0             # Engine ass√≠ncrono do Scrapy
itemadapter>=0.7.0           # Adaptador de items do Scrapy
requests>=2.31.0             # HTTP requests
urllib3>=2.0.0               # HTTP client base
python-dotenv>=1.0.0         # Vari√°veis de ambiente

# === N√çVEL 2 (Sistema de Filas) ===
celery>=5.3.0                # Framework de filas distribu√≠das
redis>=5.0.0                 # Broker de mensagens
kombu>=5.3.0                 # Biblioteca de messaging do Celery

# === DESENVOLVIMENTO ===
pytest>=7.4.0                # Framework de testes
pytest-asyncio>=0.21.0       # Suporte async para pytest
pytest-cov>=4.1.0            # Coverage de c√≥digo
pytest-mock>=3.11.0          # Mock fixtures

# === OPCIONAIS ===
flower>=2.0.0                 # Monitoramento Celery (web UI)
```

### üîß Instala√ß√£o
```bash
# 1. Clonar/baixar o projeto
cd servimed-scraper

# 2. Instalar depend√™ncias
python -m pip install -r requirements.txt

# 3. Configurar vari√°veis de ambiente (.env)
# Copiar .env.example para .env e ajustar valores
```

### üîê Vari√°veis de Ambiente (.env)
```env
# TOKENS DE AUTENTICA√á√ÉO (obrigat√≥rio - extrair do navegador)
ACCESS_TOKEN=seu_access_token_aqui
SESSION_TOKEN=seu_session_token_jwt_aqui

# CREDENCIAIS DO PORTAL (obrigat√≥rio)
PORTAL_EMAIL=seu_email@dominio.com.br
PORTAL_PASSWORD=sua_senha_portal

# CONFIGURA√á√ïES DO USU√ÅRIO (obrigat√≥rio)
LOGGED_USER=codigo_usuario
CLIENT_ID=codigo_cliente
CLIENT_SECRET=codigo_secreto_cliente
X_CART=hash_carrinho_usuario

# USU√ÅRIOS AUTORIZADOS (opcional - separados por v√≠rgula)
USERS=codigo1,codigo2,codigo3

# COTEFACIL API - OAuth2PasswordBearer (obrigat√≥rio)
COTEFACIL_BEARER_TOKEN=seu_bearer_token_cotefacil
COTEFACIL_API_URL=https://desafio.cotefacil.net

# CALLBACK API - Credenciais para sistema de callbacks (obrigat√≥rio)
CALLBACK_API_USER=seu_usuario_callback@dominio.com.br
CALLBACK_API_PASSWORD=sua_senha_callback
CALLBACK_URL=https://desafio.cotefacil.net

# URLs DO SISTEMA (configura√ß√£o padr√£o)
PORTAL_URL=https://pedidoeletronico.servimed.com.br
BASE_URL=https://peapi.servimed.com.br
API_ENDPOINT=/api/carrinho/oculto
SITE_VERSION=4.0.27

# Redis/Celery (opcional - para N√≠vel 2)
REDIS_URL=redis://localhost:6379/0
CELERY_BROKER_URL=redis://localhost:6379/0

# Logs (opcional)
LOG_LEVEL=INFO
LOG_FILE=data/logs/servimed_scraper.log
```

---

## üß™ SISTEMA DE TESTES AUTOMATIZADOS

### üìä Estat√≠sticas dos Testes
- **Status:** ‚úÖ Testes automatizados implementados
- **Funcionais:** Testes b√°sicos funcionando
- **Categorias:** Unit, Integration, Error Handling

### üîß Executando Testes

#### üéØ M√©todo 1: Script Automatizado
```bash
# Executar script interativo
python run_tests.py

# Op√ß√µes dispon√≠veis:
# 1. Executar todos os testes
# 2. Executar teste espec√≠fico
# 3. Instalar depend√™ncias de teste
# 4. Verificar estrutura de testes
# 5. Sair
```

#### üéØ M√©todo 2: Pytest Direto
```bash
# Todos os testes
python -m pytest tests/ -v

# Testes espec√≠ficos
python -m pytest tests/test_basic_functionality.py -v
python -m pytest tests/test_config.py -v
python -m pytest tests/test_main.py -v

# Testes por categoria
python -m pytest tests/ -k "test_basic" -v
python -m pytest tests/ -k "integration" -v

# Com cobertura de c√≥digo
python -m pytest tests/ --cov=src --cov=main --cov-report=html

# Apenas verificar estrutura
python -m pytest tests/ --collect-only
```

#### üéØ M√©todo 3: Testes Espec√≠ficos
```bash
# Teste de funcionalidade b√°sica (sempre funciona)
python -m pytest tests/test_basic_functionality.py::TestBasicFunctionality::test_python_environment -v

# Testes de configura√ß√£o de paths
python -m pytest tests/test_config.py::TestConfigPaths -v

# Testes do ScrapyWrapper
python -m pytest tests/test_scrapy_wrapper.py::TestScrapyWrapper::test_setup_logging -v
```

### üìã Tipos de Testes Implementados

#### üîß **Testes B√°sicos** ‚úÖ
- ‚úÖ Ambiente Python funcional
- ‚úÖ Estrutura do projeto
- ‚úÖ Importa√ß√µes de m√≥dulos
- ‚úÖ Cria√ß√£o de objetos principais
- ‚úÖ Opera√ß√µes de arquivo
- ‚úÖ Depend√™ncias instaladas

#### üîó **Testes de Configura√ß√£o** ‚úÖ
- ‚úÖ Paths do projeto
- ‚úÖ Diret√≥rios de trabalho
- ‚úÖ Arquivos de configura√ß√£o
- ‚úÖ Vari√°veis de ambiente
- ‚úÖ Integra√ß√£o de configura√ß√µes

#### üöÄ **Testes do Sistema**
- ScrapyWrapper functionality
- Sistema de filas Celery
- Cliente de pedidos
- Integra√ß√£o end-to-end
- Tratamento de erros

---

## üõ†Ô∏è GUIA DE DESENVOLVIMENTO

### üîç Debug e Logs
```bash
# Logs detalhados do Scrapy
python main.py --nivel 1 --filtro "test" --max-pages 1

# Logs do Celery (N√≠vel 2)
python -m celery -A src.nivel2.celery_app worker --loglevel=debug

# Verificar logs em arquivo
# Windows: type data\logs\servimed_scraper.log
# Linux/Mac: cat data/logs/servimed_scraper.log
```

### üîß Estrutura do C√≥digo

#### üìÑ main.py - Ponto de Entrada
```python
# Fun√ß√µes principais:
def executar_nivel_1(args)  # Execu√ß√£o direta
def executar_nivel_2(args)  # Sistema de filas  
def main()                  # Parser de argumentos e orquestra√ß√£o
```

#### üìÑ src/scrapy_wrapper.py - Wrapper Scrapy
```python
class ScrapyServimedWrapper:
    def run_spider(filtro="", max_pages=1)    # Executar spider
    def get_results()                          # Obter resultados
    def run_spider_subprocess(...)             # Execu√ß√£o via subprocess
```

#### üìÑ src/nivel2/queue_client.py - Cliente de Filas
```python
class TaskQueueClient:
    def enqueue_scraping_task(...)             # Enfileirar scraping
    def get_task_status(task_id)               # Status da tarefa
    def get_worker_status()                    # Status dos workers
```

#### üìÑ src/pedido_queue_client.py - Cliente de Pedidos
```python
class PedidoQueueClient:
    def enqueue_pedido(...)                    # Enfileirar pedido
    def get_status(task_id)                    # Status do pedido
```

### üìä Fluxo de Dados

#### N√≠vel 1: Direto
```
main.py ‚Üí ScrapyWrapper ‚Üí Scrapy Spider ‚Üí JSON File ‚Üí Results
```

#### N√≠vel 2: Filas
```
main.py ‚Üí TaskQueueClient ‚Üí Celery Task ‚Üí ScrapyWrapper ‚Üí Callback API
```

#### N√≠vel 3: Pedidos
```
pedido_queue_client.py ‚Üí Celery Task ‚Üí Order Processing ‚Üí External API
```

---

## üö® TROUBLESHOOTING

### ‚ùå Problemas Comuns

#### 1. **Redis n√£o est√° rodando**
```bash
# Erro: ConnectionError: Error 10061 connecting to localhost:6379
# Solu√ß√£o: Iniciar Redis
docker run -d -p 6379:6379 redis:latest
```

#### 2. **Workers Celery n√£o ativos**
```bash
# Erro: No workers available
# Solu√ß√£o: Iniciar worker
python -m celery -A src.nivel2.celery_app worker --loglevel=info
```

#### 3. **Depend√™ncias faltando**
```bash
# Erro: ModuleNotFoundError
# Solu√ß√£o: Instalar depend√™ncias
python -m pip install -r requirements.txt
```

#### 4. **Testes falhando**
```bash
# Verificar ambiente b√°sico primeiro
python -m pytest tests/test_basic_functionality.py -v

# Se b√°sicos passarem, problema √© nos mocks/imports espec√≠ficos
```

#### 5. **Arquivo .env n√£o configurado**
```bash
# Criar .env com vari√°veis necess√°rias
# Ver se√ß√£o "Vari√°veis de Ambiente" acima
```

### üîß Debug Mode
```bash
# Scrapy com debug
python -c "
import sys
sys.path.insert(0, 'src')
from scrapy_wrapper import ScrapyServimedWrapper
wrapper = ScrapyServimedWrapper()
print('Wrapper criado com sucesso!')
"

# Celery com debug
python -c "
import sys
sys.path.insert(0, 'src')
from nivel2.queue_client import TaskQueueClient
client = TaskQueueClient()
print('Cliente criado com sucesso!')
"
```

---

## üìà M√âTRICAS E PERFORMANCE

### üîç Monitoramento
```bash
# Status do sistema
python main.py --nivel 2 --worker-status

# Logs em tempo real
# Windows PowerShell: Get-Content data\logs\servimed_scraper.log -Wait
# Linux/Mac: tail -f data/logs/servimed_scraper.log
```

---

## üîß COMANDOS ESSENCIAIS

### üìä Para Desenvolvimento
```bash
# Executar scraping b√°sico
python main.py --nivel 1 --max-pages 1

# Executar testes b√°sicos
python -m pytest tests/test_basic_functionality.py -v

# Ver estrutura de testes
python -m pytest tests/ --collect-only

# Executar teste espec√≠fico
python -m pytest tests/test_config.py::TestConfigPaths::test_project_root_path -v
```

### üö® Para Troubleshooting
```bash
# Verificar se arquivos existem
# Windows: dir main.py / dir src\scrapy_wrapper.py / dir tests\test_basic_functionality.py
# Linux/Mac: ls main.py / ls src/scrapy_wrapper.py / ls tests/test_basic_functionality.py

# Testar imports manualmente
python -c "import main; print('main.py OK')"
python -c "import sys; sys.path.insert(0, 'src'); import scrapy_wrapper; print('scrapy_wrapper OK')"

# Ver conte√∫do de arquivos importantes
# Windows: type pytest.ini / type requirements.txt
# Linux/Mac: cat pytest.ini / cat requirements.txt
```

### üìà Para Produ√ß√£o (N√≠veis 2 e 3)
```bash
# N√≠vel 2 - Setup (requer Redis)
# 1. Instalar Redis: docker run -d -p 6379:6379 redis:latest
# 2. Iniciar worker: python -m celery -A src.nivel2.celery_app worker --loglevel=info
# 3. Enfileirar: python main.py --nivel 2 --enqueue --usuario "test@example.com" --senha "password"

# N√≠vel 3 - Pedidos
python src/pedido_queue_client.py test
python main.py --nivel 3
```

---

## ‚úÖ CHECKLIST DE VERIFICA√á√ÉO

### üß™ Checklist de Funcionalidades

#### ‚úÖ Core Functionalities
- [x] **Scraping B√°sico** - N√≠vel 1 funcionando
- [x] **Testes Automatizados** - Testes b√°sicos implementados
- [x] **Estrutura Modular** - Imports e m√≥dulos OK
- [x] **Configura√ß√µes** - Paths e settings funcionando
- [x] **Logging** - Sistema de logs ativo
- [x] **Error Handling** - Tratamento de erros implementado

#### üîÑ Advanced Features (configura√ß√£o adicional necess√°ria)
- [ ] **Sistema de Filas** - Requer Redis + Celery workers
- [ ] **API de Pedidos** - Requer configura√ß√£o de credenciais
- [ ] **Callbacks HTTP** - Requer endpoints externos
- [ ] **Monitoramento** - Requer dashboard setup

#### üß™ Quality Assurance
- [x] **Unit Tests** - Testes unit√°rios funcionando
- [x] **Integration Tests** - Testes b√°sicos de integra√ß√£o
- [x] **Environment Tests** - Valida√ß√£o de ambiente
- [x] **Configuration Tests** - Testes de configura√ß√£o
- [x] **Documentation** - Documenta√ß√£o completa


---


## üìû SUPORTE R√ÅPIDO

### ‚ùå Se algo n√£o funcionar:
1. **Execute primeiro:** `python -m pytest tests/test_basic_functionality.py::TestBasicFunctionality::test_python_environment -v`
2. **Se falhar:** Problema no ambiente Python
3. **Se passar:** Execute teste completo: `python -m pytest tests/test_basic_functionality.py -v`
4. **Para debug:** Use comandos da se√ß√£o troubleshooting

### ‚úÖ Se tudo funcionar:
1. **Continue** com scraping b√°sico: `python main.py --nivel 1 --max-pages 1`
2. **Explore** filtros: `python main.py --nivel 1 --filtro "seu_termo" --max-pages 2`
3. **Avance** para n√≠veis superiores conforme necessidade

### üìö Recursos Adicionais
- **Scrapy Docs:** https://docs.scrapy.org/
- **Celery Docs:** https://docs.celeryproject.org/
- **Pytest Docs:** https://docs.pytest.org/

