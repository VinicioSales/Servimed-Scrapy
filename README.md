# Servimed Scraper

Sistema completo de scraping e pedidos para o portal Servimed, desenvolvido com Scrapy framework.

## ğŸ“ Estrutura do Projeto

```
servimed-scraper/
â”œâ”€â”€ main.py                       # Script principal - 3 nÃ­veis
â”œâ”€â”€ pedido_queue_client.py        # Cliente de pedidos (nÃ­vel 3)
â”œâ”€â”€ queue_client.py               # Cliente de filas (nÃ­vel 2)
â”œâ”€â”€ scrapy.cfg                    # ConfiguraÃ§Ã£o do Scrapy
â”œâ”€â”€ requirements.txt              # DependÃªncias Python
â”œâ”€â”€ .env                          # VariÃ¡veis de ambiente (nÃ£o versionado)
â”œâ”€â”€ .env.example                  # Exemplo de configuraÃ§Ã£o
â”‚
â”œâ”€â”€ src/                          # CÃ³digo fonte principal
â”‚   â”œâ”€â”€ scrapy_wrapper.py         # Wrapper para Scrapy
â”‚   â”œâ”€â”€ api_client/               # Clientes de API
â”‚   â”œâ”€â”€ nivel2/                   # Sistema de filas (Celery)
â”‚   â”œâ”€â”€ nivel3/                   # Sistema de pedidos
â”‚   â”œâ”€â”€ scrapy_servimed/          # Spiders Scrapy
â”‚   â””â”€â”€ servimed_scraper/         # Scraper original
â”‚
â”œâ”€â”€ config/                       # ConfiguraÃ§Ãµes
â”œâ”€â”€ data/                         # Dados coletados
â”œâ”€â”€ docs/                         # DocumentaÃ§Ã£o
â”œâ”€â”€ logs/                         # Arquivos de log
â”œâ”€â”€ scripts/                      # Scripts auxiliares
â”œâ”€â”€ tests/                        # Testes automatizados
â””â”€â”€ tools/                        # Ferramentas auxiliares
```
â”‚   â”œâ”€â”€ pedido_queue_client.py    # Cliente de pedidos (NÃ­vel 3)
â”‚   â””â”€â”€ queue_client.py           # Cliente de filas (NÃ­vel 2)
â”œâ”€â”€ src/                          # CÃ³digo fonte do sistema
â”‚   â”œâ”€â”€ api_client/               # Clientes para APIs externas
â”‚   â”œâ”€â”€ nivel2/                   # Sistema de filas (Celery/Redis)
â”‚   â”œâ”€â”€ nivel3/                   # Sistema de pedidos
â”‚   â”œâ”€â”€ scrapy_servimed/          # Spiders do Scrapy
â”‚   â””â”€â”€ scrapy_wrapper.py         # Wrapper do Scrapy
â”œâ”€â”€ scripts/                      # Scripts de utilitÃ¡rios
â”‚   â”œâ”€â”€ start_worker.bat          # Inicia worker Celery
â”‚   â””â”€â”€ redis_start.bat           # Inicia Redis
â”œâ”€â”€ tools/                        # Ferramentas auxiliares
â”‚   â””â”€â”€ redis-portable/           # Redis portÃ¡vel
â”œâ”€â”€ config/                       # ConfiguraÃ§Ãµes do sistema
â”œâ”€â”€ data/                         # Dados gerados pelo scraping
â”œâ”€â”€ docs/                         # DocumentaÃ§Ã£o
â”œâ”€â”€ logs/                         # Logs do sistema
â”œâ”€â”€ tests/                        # Testes unitÃ¡rios
â”œâ”€â”€ servimed.bat                  # ExecutÃ¡vel principal (Windows)
â”œâ”€â”€ pedidos.bat                   # ExecutÃ¡vel de pedidos (Windows)
â”œâ”€â”€ .env                          # VariÃ¡veis de ambiente
â”œâ”€â”€ requirements.txt              # DependÃªncias Python
â””â”€â”€ README.md                     # Este arquivo
```

## ğŸš€ InstalaÃ§Ã£o

1. **Clone o repositÃ³rio:**
   ```bash
   git clone <url-do-repositorio>
   cd servimed-scraper
   ```

2. **Instale as dependÃªncias:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Configure o ambiente:**
   ```bash
   cp .env.example .env
   # Edite o .env com suas credenciais
   ```

## ğŸ¯ Uso

### NÃ­vel 1 - ExecuÃ§Ã£o Direta (SÃ­ncrona)

ExecuÃ§Ã£o simples e direta do scraper:

```bash
# Executar scraping bÃ¡sico
python main.py --nivel 1

# Com filtro de produto
python main.py --nivel 1 --filtro "paracetamol"

# Limitando pÃ¡ginas
python main.py --nivel 1 --max-pages 5
```

```bash
# Windows
servimed.bat --nivel 1 --filtro "paracetamol"

# Linux/Mac  
python bin/main.py --nivel 1 --filtro "paracetamol"
```

### NÃ­vel 2 - Sistema de Filas (AssÃ­ncrona)

1. **Inicie o Redis:**
   ```bash
   scripts/redis_start.bat
   ```

2. **Inicie o Worker:**
   ```bash
   scripts/start_worker.bat
   ```

3. **Enfileire tarefas:**
   ```bash
   python bin/main.py --nivel 2 --enqueue --filtro "dipirona"
   ```

### NÃ­vel 3 - Sistema de Pedidos

```bash
# Teste do sistema
pedidos.bat test

# Criar pedido
pedidos.bat enqueue PEDIDO001 444212 2

# Verificar status
pedidos.bat status <task_id>
```

## ğŸ”§ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente (.env)

```env
# AutenticaÃ§Ã£o Servimed
SERVIMED_EMAIL=seu_email@exemplo.com
SERVIMED_SENHA=sua_senha

# API Cotefacil
CALLBACK_API_USER=usuario_api
CALLBACK_API_PASSWORD=senha_api
CALLBACK_URL=https://desafio.cotefacil.net

# Tokens de sessÃ£o (extraÃ­dos do navegador)
ACCESS_TOKEN=seu_access_token
SESSION_TOKEN=seu_session_token
LOGGED_USER=seu_user_id
CLIENT_ID=seu_client_id
X_CART=seu_x_cart
```

## ğŸ—ï¸ Arquitetura

### Framework Ãšnico: Scrapy
- Todos os nÃ­veis usam Scrapy automaticamente
- Sistema otimizado e padronizado
- Melhor performance e manutenibilidade

### NÃ­veis de Funcionamento:

1. **NÃ­vel 1**: ExecuÃ§Ã£o direta com Scrapy
2. **NÃ­vel 2**: Sistema de filas com Celery + Redis + Scrapy  
3. **NÃ­vel 3**: Sistema completo de pedidos + Scrapy

### Componentes Principais:

- **Scrapy Spiders**: Coleta de dados do portal
- **Celery Tasks**: Processamento assÃ­ncrono
- **API Client**: IntegraÃ§Ã£o com APIs externas
- **Queue System**: Gerenciamento de filas Redis

## ğŸ“Š Dados Coletados

Para cada produto:
- GTIN (CÃ³digo de barras)
- CÃ³digo interno
- DescriÃ§Ã£o
- PreÃ§o de fÃ¡brica
- Estoque disponÃ­vel

Formatos de saÃ­da:
- JSON estruturado
- Logs detalhados
- RelatÃ³rios de status

## ğŸ› ï¸ Desenvolvimento

### Estrutura de Desenvolvimento:
```bash
# Instalar em modo desenvolvimento
pip install -e .

# Executar testes
python -m pytest tests/

# Verificar qualidade do cÃ³digo
flake8 src/
```

### Contribuindo:
1. Fork o projeto
2. Crie uma branch para sua feature
3. FaÃ§a commit das mudanÃ§as
4. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo LICENSE para detalhes.
