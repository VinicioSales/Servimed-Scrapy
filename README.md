# SERVIMED SCRAPER - DOCUMENTA√á√ÉO COMPLETA
=============================================

## üìã √çNDICE
- [Vis√£o Geral](#vis√£o-geral)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [Instala√ß√£o](#instala√ß√£o)
- [Configura√ß√£o](#configura√ß√£o)
- [Modo de Uso](#modo-de-uso)
- [N√≠vel 1: Execu√ß√£o Direta](#n√≠vel-1-execu√ß√£o-direta)
- [N√≠vel 2: Sistema de Filas](#n√≠vel-2-sistema-de-filas)
- [Troubleshooting](#troubleshooting)
- [Desenvolvimento](#desenvolvimento)

---

## üéØ VIS√ÉO GERAL

O Servimed Scraper √© uma aplica√ß√£o Python que coleta produtos do Portal Servimed com suporte a dois modos de opera√ß√£o:

- **N√≠vel 1**: Execu√ß√£o direta e s√≠ncrona (modo original)
- **N√≠vel 2**: Sistema de filas ass√≠ncronas com integra√ß√£o a API externa

### Caracter√≠sticas Principais
- ‚úÖ Coleta completa de produtos Servimed
- ‚úÖ Filtros de busca personaliz√°veis  
- ‚úÖ Sistema de backup autom√°tico
- ‚úÖ Processamento ass√≠ncrono com Celery
- ‚úÖ Integra√ß√£o OAuth2 com API externa
- ‚úÖ Monitoramento de tarefas em tempo real

---

## üìÅ ESTRUTURA DO PROJETO

```
PROVA/
‚îú‚îÄ‚îÄ main.py                          # Script principal
‚îú‚îÄ‚îÄ requirements.txt                 # Depend√™ncias
‚îú‚îÄ‚îÄ .env.example                     # Template de configura√ß√£o
‚îú‚îÄ‚îÄ README.md                        # Esta documenta√ß√£o
‚îú‚îÄ‚îÄ data/                           # Arquivos de sa√≠da
‚îÇ   ‚îú‚îÄ‚îÄ servimed_produtos_completos.json
‚îÇ   ‚îú‚îÄ‚îÄ servimed_produtos_filtrados.json
‚îÇ   ‚îî‚îÄ‚îÄ servimed_backup.json
‚îî‚îÄ‚îÄ src/
    ‚îú‚îÄ‚îÄ servimed_scraper/           # M√≥dulo principal de scraping
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îú‚îÄ‚îÄ servimed_scraper_completo.py
    ‚îÇ   ‚îú‚îÄ‚îÄ auth_manager.py
    ‚îÇ   ‚îî‚îÄ‚îÄ data_processor.py
    ‚îú‚îÄ‚îÄ nivel2/                     # Sistema de filas (N√≠vel 2)
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îú‚îÄ‚îÄ celery_app.py
    ‚îÇ   ‚îú‚îÄ‚îÄ tasks.py
    ‚îÇ   ‚îî‚îÄ‚îÄ queue_client.py
    ‚îî‚îÄ‚îÄ api_client/                 # Cliente da API externa
        ‚îú‚îÄ‚îÄ __init__.py
        ‚îî‚îÄ‚îÄ callback_client.py
```

---

## üöÄ INSTALA√á√ÉO

### Pr√©-requisitos
- Python 3.8 ou superior
- Redis Server (para N√≠vel 2)
- Git (opcional)

### 1. Clone ou baixe o projeto
```bash
# Se usando Git
git clone <url-do-repositorio>
cd PROVA

# Ou extraia o arquivo ZIP na pasta desejada
```

### 2. Instale as depend√™ncias
```bash
# Instalar todas as depend√™ncias
pip install -r requirements.txt

# OU instalar apenas o essencial para N√≠vel 1
pip install scrapy selenium beautifulsoup4 requests pandas numpy
```

### 3. Configura√ß√£o inicial
```bash
# Copie o template de configura√ß√£o
copy .env.example .env

# Edite o arquivo .env com seus dados
notepad .env
```

---

## ‚öôÔ∏è CONFIGURA√á√ÉO

### 1. Obter Tokens de Autentica√ß√£o

Para usar o scraper, voc√™ precisa dos tokens do Portal Servimed:

1. **Abra o Portal Servimed** no navegador
2. **Fa√ßa login** com suas credenciais
3. **Abra DevTools** (F12) > Aba Network
4. **Fa√ßa uma busca** qualquer no portal
5. **Encontre a requisi√ß√£o "oculto"** na lista
6. **Copie os valores** dos headers/cookies:

```
ACCESS_TOKEN=xxxxxxxxxxxx
SESSION_TOKEN=yyyyyyyyyyyy
LOGGED_USER=zzzzzzzzzzzz
CLIENT_ID=wwwwwwwwwwww
X_CART=vvvvvvvvvvvv
```

### 2. Configurar arquivo .env

Edite o arquivo `.env` com seus dados:

```bash
# Tokens obrigat√≥rios (obtidos no passo anterior)
ACCESS_TOKEN=seu_token_access_real
SESSION_TOKEN=seu_token_session_real
LOGGED_USER=seu_user_id_real
CLIENT_ID=seu_client_id_real
X_CART=seu_x_cart_real

# Credenciais do portal
PORTAL_EMAIL=seu_email@exemplo.com
PORTAL_PASSWORD=sua_senha_real

# Usu√°rios autorizados
USERS=user1,user2,user3
```

### 3. Configura√ß√£o para N√≠vel 2 (Opcional)

Se voc√™ planeja usar o sistema de filas:

```bash
# API de callback
CALLBACK_API_URL=https://desafio.cotefacil.net
CALLBACK_API_USER=juliano@farmaprevonline.com.br
CALLBACK_API_PASSWORD=a007299A

# Redis (mantenha padr√£o se local)
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
```

---

## üîß MODO DE USO

### Verificar Instala√ß√£o
```bash
# Teste b√°sico
python main.py --help

# Deve mostrar todas as op√ß√µes dispon√≠veis
```

---

## üìä N√çVEL 1: EXECU√á√ÉO DIRETA

O N√≠vel 1 √© o modo **s√≠ncrono** original - execu√ß√£o direta e imediata.

### Comandos B√°sicos

```bash
# Coletar TODOS os produtos
python main.py --nivel 1

# Coletar com filtro
python main.py --nivel 1 --filtro "paracetamol"

# Limitar n√∫mero de p√°ginas
python main.py --nivel 1 --max-pages 10

# Combinar filtro + limite
python main.py --nivel 1 --filtro "dipirona" --max-pages 5
```

### Exemplos Pr√°ticos

```bash
# Medicamentos para dor
python main.py --nivel 1 --filtro "paracetamol" --max-pages 3

# Antibi√≥ticos
python main.py --nivel 1 --filtro "amoxicilina" --max-pages 2

# Coleta completa (cuidado: pode demorar horas)
python main.py --nivel 1
```

### Arquivos de Sa√≠da

- `data/servimed_produtos_completos.json` - Todos os produtos
- `data/servimed_produtos_filtrados.json` - Produtos filtrados
- `data/servimed_backup.json` - Backup autom√°tico

---

## üîÑ N√çVEL 2: SISTEMA DE FILAS

O N√≠vel 2 usa **filas ass√≠ncronas** com Celery + Redis para processamento em background e integra√ß√£o com API externa.

### Pr√©-requisitos para N√≠vel 2

#### 1. Instalar Redis
```bash
# Windows (usando Chocolatey)
choco install redis-64

# OU baixar do site oficial
# https://redis.io/download

# Linux Ubuntu/Debian
sudo apt install redis-server

# macOS
brew install redis
```

#### 2. Iniciar Redis
```bash
# Windows
redis-server

# Linux/macOS
redis-server
# OU
sudo systemctl start redis
```

#### 3. Verificar Redis
```bash
redis-cli ping
# Deve retornar: PONG
```

### Executar Workers Celery

Em um **terminal separado**, inicie o worker:

```bash
# Navegar para o diret√≥rio do projeto
cd C:\Users\6128347\OneDrive - Thomson Reuters Incorporated\Documents\Scrips\Tests\PROVA

# Iniciar worker
celery -A src.nivel2.celery_app worker --loglevel=info

# No Windows, se der erro, usar:
celery -A src.nivel2.celery_app worker --loglevel=info --pool=solo
```

### Comandos do N√≠vel 2

#### Enfileirar Tarefas
```bash
# Enfileirar tarefa b√°sica
python main.py --nivel 2 --enqueue

# Com filtro e limite
python main.py --nivel 2 --enqueue --filtro "paracetamol" --max-pages 3

# Com credenciais personalizadas
python main.py --nivel 2 --enqueue --usuario "seu@email.com" --senha "suasenha"
```

#### Monitorar Tarefas
```bash
# Status de uma tarefa espec√≠fica
python main.py --nivel 2 --status 12345678-1234-1234-1234-123456789abc

# Status dos workers
python main.py --nivel 2 --worker-status
```

### Fluxo Completo do N√≠vel 2

1. **Enfileirar tarefa**:
   ```bash
   python main.py --nivel 2 --enqueue --filtro "dipirona"
   # Retorna: Task ID: abc123-def456-ghi789
   ```

2. **Monitorar progresso**:
   ```bash
   python main.py --nivel 2 --status abc123-def456-ghi789
   ```

3. **Ver logs do worker** no terminal onde est√° executando

4. **Verificar resultado** - produtos s√£o enviados automaticamente para a API

### Monitoramento com Flower (Opcional)

Para interface web de monitoramento:

```bash
# Instalar Flower
pip install flower

# Iniciar Flower
celery -A src.nivel2.celery_app flower

# Acessar: http://localhost:5555
```

---

## üîç TROUBLESHOOTING

### Problemas Comuns

#### 1. Erro de Import
```
ImportError: No module named 'servimed_scraper'
```
**Solu√ß√£o**: Verificar se est√° executando da pasta raiz do projeto

#### 2. Tokens Inv√°lidos
```
Erro 401: Unauthorized
```
**Solu√ß√£o**: Atualizar tokens no arquivo `.env`

#### 3. Redis Connection Error (N√≠vel 2)
```
ConnectionError: Error connecting to Redis
```
**Solu√ß√£o**:
```bash
# Verificar se Redis est√° rodando
redis-cli ping

# Iniciar Redis se necess√°rio
redis-server
```

#### 4. Celery Worker Error (N√≠vel 2)
```
ValueError: not enough values to unpack
```
**Solu√ß√£o**: No Windows, usar `--pool=solo`:
```bash
celery -A src.nivel2.celery_app worker --loglevel=info --pool=solo
```

#### 5. Erro de Autentica√ß√£o na API (N√≠vel 2)
```
OAuth2 authentication failed
```
**Solu√ß√£o**: Verificar credenciais no `.env`:
```bash
CALLBACK_API_USER=email_correto@dominio.com
CALLBACK_API_PASSWORD=senha_correta
```

### Logs e Debug

#### Habilitar Modo Verbose
Edite `.env`:
```bash
DEBUG=true
VERBOSE=true
LOG_LEVEL=DEBUG
```

#### Verificar Logs do Celery
```bash
# Worker com mais detalhes
celery -A src.nivel2.celery_app worker --loglevel=debug

# Logs em arquivo
celery -A src.nivel2.celery_app worker --loglevel=info --logfile=celery.log
```

---

## üë®‚Äçüíª DESENVOLVIMENTO

### Estrutura de C√≥digo

#### Adicionar Novas Funcionalidades

1. **Novo filtro de produtos**:
   - Editar `src/servimed_scraper/servimed_scraper_completo.py`
   - M√©todo `processar_dados()`

2. **Nova API de destino**:
   - Criar novo cliente em `src/api_client/`
   - Registrar em `src/nivel2/tasks.py`

3. **Nova tarefa ass√≠ncrona**:
   - Adicionar em `src/nivel2/tasks.py`
   - Atualizar `src/nivel2/queue_client.py`

### Testes

```bash
# Instalar depend√™ncias de teste
pip install pytest pytest-asyncio

# Executar testes (quando implementados)
pytest tests/

# Teste manual b√°sico
python -c "from src.servimed_scraper.servimed_scraper_completo import ServimedScraperCompleto; print('Import OK')"
```

### Contribui√ß√£o

1. Mantenha a estrutura modular
2. Adicione documenta√ß√£o para novas features
3. Teste tanto N√≠vel 1 quanto N√≠vel 2
4. Atualize requirements.txt se necess√°rio

---

## üìû SUPORTE

Para d√∫vidas ou problemas:

1. **Verificar esta documenta√ß√£o** primeiro
2. **Consultar logs** do Celery/Redis
3. **Testar configura√ß√£o** passo a passo
4. **Verificar tokens** do Portal Servimed

---

## üìÑ LICEN√áA

Este projeto √© desenvolvido para uso interno e educacional.

**Vers√£o da Documenta√ß√£o**: 1.0
**√öltima Atualiza√ß√£o**: 12/08/2025
**Desenvolvido por**: GitHub Copilot
