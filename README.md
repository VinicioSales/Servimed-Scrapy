# ğŸ“– SERVIMED SCRAPER - DOCUMENTAÃ‡ÃƒO COMPLETA

## ğŸ—ï¸ VISÃƒO GERAL

O **Servimed Scraper** Ã© um sistema completo de web scraping desenvolvido em Python para extrair informaÃ§Ãµes de produtos do site Servimed. O projeto implementa uma arquitetura em 3 nÃ­veis de complexidade, sempre utilizando o framework **Scrapy** como base.

### ğŸ¯ OBJETIVOS
- âœ… Extrair dados de produtos farmacÃªuticos do site Servimed
- âœ… Oferecer mÃºltiplas formas de execuÃ§Ã£o (sÃ­ncrona e assÃ­ncrona)
- âœ… Implementar sistema de filas para processamento em larga escala
- âœ… Fornecer API para integraÃ§Ã£o com sistemas externos
- âœ… Garantir qualidade atravÃ©s de testes automatizados

### ğŸ“Š STATUS DO PROJETO
- **Data:** 13 de Agosto de 2025
- **Status:** âœ… **COMPLETO E FUNCIONAL**
- **Framework:** Scrapy 2.13.3
- **Testes:** âœ… Sistema de testes automatizados implementado
- **Arquitetura:** 3 NÃ­veis de Complexidade

---

## ğŸ›ï¸ ARQUITETURA DO SISTEMA

### ğŸ“Š Estrutura de DiretÃ³rios
```
PROVA/
â”œâ”€â”€ ğŸ“„ main.py                      # Arquivo principal - ponto de entrada
â”œâ”€â”€ ğŸ“„ requirements.txt             # DependÃªncias do projeto
â”œâ”€â”€ ğŸ“„ .env                         # VariÃ¡veis de ambiente (configurar)
â”œâ”€â”€ ğŸ“„ pyproject.toml               # ConfiguraÃ§Ã£o do projeto (pytest, coverage)
â”œâ”€â”€ ğŸ“„ scrapy.cfg                   # ConfiguraÃ§Ã£o do Scrapy
â”œâ”€â”€ ğŸ“„ run_tests.py                 # Script para executar testes
â”œâ”€â”€ ğŸ“„ README.md                    # Esta documentaÃ§Ã£o
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ“ src/                        # CÃ³digo fonte principal
â”‚   â”œâ”€â”€ ğŸ“„ scrapy_wrapper.py       # Wrapper principal do Scrapy
â”‚   â”œâ”€â”€ ğŸ“„ pedido_queue_client.py  # Cliente para pedidos (NÃ­vel 3)
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ config/                 # ConfiguraÃ§Ãµes do sistema
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ settings.py         # ConfiguraÃ§Ãµes internas da aplicaÃ§Ã£o
â”‚   â”‚   â””â”€â”€ ğŸ“„ paths.py            # Caminhos e diretÃ³rios do projeto
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ nivel2/                 # Sistema de filas (NÃ­vel 2)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ queue_client.py     # Cliente de filas
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ tasks.py            # Tarefas Celery
â”‚   â”‚   â””â”€â”€ ğŸ“„ celery_app.py       # ConfiguraÃ§Ã£o Celery
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ scrapy_servimed/        # Projeto Scrapy
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ settings.py         # ConfiguraÃ§Ãµes Scrapy
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ pipelines.py        # Pipelines de processamento
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ items.py            # DefiniÃ§Ã£o de items
â”‚   â”‚   â””â”€â”€ ğŸ“ spiders/            # Spiders de scraping
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚       â””â”€â”€ ğŸ“„ servimed_spider.py  # Spider principal
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ servimed_scraper/       # Sistema de processamento
â”‚       â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”œâ”€â”€ ğŸ“„ celery_app.py       # App Celery principal
â”‚       â””â”€â”€ ğŸ“„ tasks.py            # Tarefas de processamento
â”‚
â”œâ”€â”€ ğŸ“ tests/                      # Testes automatizados
â”‚   â”œâ”€â”€ ğŸ“„ conftest.py             # ConfiguraÃ§Ãµes globais de teste
â”‚   â”œâ”€â”€ ğŸ“„ test_basic_functionality.py  # Testes bÃ¡sicos âœ…
â”‚   â”œâ”€â”€ ğŸ“„ test_config.py          # Testes de configuraÃ§Ã£o
â”‚   â”œâ”€â”€ ğŸ“„ test_main.py            # Testes do main.py
â”‚   â”œâ”€â”€ ğŸ“„ test_scrapy_wrapper.py  # Testes do wrapper
â”‚   â”œâ”€â”€ ğŸ“„ test_spiders.py         # Testes dos spiders
â”‚   â”œâ”€â”€ ğŸ“„ test_integration.py     # Testes de integraÃ§Ã£o
â”‚   â”œâ”€â”€ ğŸ“ test_nivel2/
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_tasks.py       # Testes Celery
â”‚   â””â”€â”€ ğŸ“ test_nivel3/
â”‚       â””â”€â”€ ğŸ“„ test_pedido_queue_client.py  # Testes pedidos
â”‚
â”œâ”€â”€ ğŸ“ data/                       # Dados e resultados
â”‚   â”œâ”€â”€ ğŸ“„ servimed_produtos_scrapy.json   # Resultados Scrapy
â”‚   â””â”€â”€ ğŸ“„ logs/                   # Logs do sistema

```

---

## ğŸš€ INÃCIO RÃPIDO

### ğŸ“‹ PrÃ©-requisitos
- âœ… Python 3.10+
- âœ… ConexÃ£o com Internet

### ğŸ”§ Setup RÃ¡pido

#### 1. Verificar se estÃ¡ tudo funcionando:
```bash
# Navegar para o diretÃ³rio do projeto
cd servimed-scraper

# Testar ambiente bÃ¡sico
python -m pytest tests/test_basic_functionality.py::TestBasicFunctionality::test_python_environment -v
```

#### 2. Executar primeiro teste do sistema:
```bash
# NÃ­vel 1 - ExecuÃ§Ã£o direta (mais simples)
python main.py --nivel 1 --filtro "paracetamol" --max-pages 1
```

#### 3. Verificar resultados:
```bash
# Ver arquivo gerado
type data\servimed_produtos_scrapy.json
```

### ğŸ“ Exemplos PrÃ¡ticos

#### ğŸ¯ Exemplo 1: Busca Simples
```bash
# Buscar produtos com "dipirona" limitado a 2 pÃ¡ginas
python main.py --nivel 1 --filtro "dipirona" --max-pages 2

# Resultado esperado: Arquivo JSON com produtos encontrados
```

#### ğŸ¯ Exemplo 2: Executar Testes
```bash
# Testes bÃ¡sicos (sempre funcionam)
python -m pytest tests/test_basic_functionality.py -v
```

#### ğŸ¯ Exemplo 3: Script Interativo de Testes
```bash
# Executar menu interativo
python run_tests.py

# Escolher opÃ§Ã£o 1: Executar todos os testes
# Escolher opÃ§Ã£o 4: Verificar estrutura
```

---

## ğŸ¯ NÃVEIS DE EXECUÃ‡ÃƒO

### ğŸ¯ NÃVEL 1: EXECUÃ‡ÃƒO DIRETA (SÃNCRONA)
**DescriÃ§Ã£o:** ExecuÃ§Ã£o direta e imediata usando Scrapy  

#### ğŸ”§ Como Usar:
```bash
# ExecuÃ§Ã£o bÃ¡sica
python main.py --nivel 1

# Com filtro de produtos
python main.py --nivel 1 --filtro "paracetamol"

# Limitando pÃ¡ginas
python main.py --nivel 1 --max-pages 5

# Combinando opÃ§Ãµes
python main.py --nivel 1 --filtro "dipirona" --max-pages 3
```

#### ğŸ“Š SaÃ­da:
- Arquivo: `data/servimed_produtos_scrapy.json`
- Format: JSON com lista de produtos
- Logs: Console em tempo real

---

### ğŸ¯ NÃVEL 2: SISTEMA DE FILAS (ASSÃNCRONA)
**DescriÃ§Ã£o:** Processamento via filas usando Celery + Redis  
**Uso:** Para processamento em larga escala e produÃ§Ã£o  
**Framework:** Scrapy + Celery + Redis

#### ğŸ”§ PrÃ©-requisitos:
```bash
# 1. Instalar e iniciar Redis
# Windows: Baixar Redis ou usar Docker
docker run -d -p 6379:6379 redis:latest

# 2. Iniciar worker Celery
python -m celery -A src.nivel2.celery_app worker --loglevel=info
```

#### ğŸ”§ Como Usar:
```bash
# Verificar status dos workers
python main.py --nivel 2 --worker-status

# Enfileirar nova tarefa
python main.py --nivel 2 --enqueue --usuario "user@example.com" --senha "password"

# Verificar status de tarefa
python main.py --nivel 2 --status "task-id-aqui"

# Com filtro
python main.py --nivel 2 --enqueue --filtro "antibiotico"
```

#### ğŸ“Š SaÃ­da:
- Task ID para acompanhamento
- Callback HTTP para notificaÃ§Ã£o
- Resultados via API status

---

### ğŸ¯ NÃVEL 3: SISTEMA DE PEDIDOS
**DescriÃ§Ã£o:** Sistema completo de processamento de pedidos  
**Framework:** Scrapy + Celery + Sistema de Pedidos

#### ğŸ”§ Como Usar:
```bash
# Teste do sistema
python src/pedido_queue_client.py test

# Enfileirar pedido
python src/pedido_queue_client.py enqueue "PEDIDO123" "444212" "2" "1234567890123"

# Verificar status do pedido
python src/pedido_queue_client.py status "task-id"

# Via main.py (orientaÃ§Ãµes)
python main.py --nivel 3
```

#### ğŸ“Š SaÃ­da:
- Task ID do pedido
- Status detalhado via API
- IntegraÃ§Ã£o com sistemas externos

---

## âš™ï¸ CONFIGURAÃ‡ÃƒO DO AMBIENTE

### ğŸ“‹ DependÃªncias Principais
```txt
# === CORE (obrigatÃ³rias) ===
scrapy>=2.11.0              # Framework de scraping principal
twisted>=22.10.0             # Engine assÃ­ncrono do Scrapy
itemadapter>=0.7.0           # Adaptador de items do Scrapy
requests>=2.31.0             # HTTP requests
urllib3>=2.0.0               # HTTP client base
python-dotenv>=1.0.0         # VariÃ¡veis de ambiente

# === NÃVEL 2 (Sistema de Filas) ===
celery>=5.3.0                # Framework de filas distribuÃ­das
redis>=5.0.0                 # Broker de mensagens
kombu>=5.3.0                 # Biblioteca de messaging do Celery

# === DESENVOLVIMENTO ===
pytest>=7.4.0                # Framework de testes
pytest-asyncio>=0.21.0       # Suporte async para pytest
pytest-cov>=4.1.0            # Coverage de cÃ³digo
pytest-mock>=3.11.0          # Mock fixtures

# === OPCIONAIS ===
flower>=2.0.0                 # Monitoramento Celery (web UI)
```

### ğŸ”§ InstalaÃ§Ã£o
```bash
# 1. Clonar/baixar o projeto
cd servimed-scraper

# 2. Instalar dependÃªncias
python -m pip install -r requirements.txt

# 3. Configurar variÃ¡veis de ambiente (.env)
# Copiar .env.example para .env e ajustar valores
```

### ğŸ” VariÃ¡veis de Ambiente (.env)
```env
# TOKENS DE AUTENTICAÃ‡ÃƒO (obrigatÃ³rio - extrair do navegador)
ACCESS_TOKEN=seu_access_token_aqui
SESSION_TOKEN=seu_session_token_jwt_aqui

# CREDENCIAIS DO PORTAL (obrigatÃ³rio)
PORTAL_EMAIL=seu_email@dominio.com.br
PORTAL_PASSWORD=sua_senha_portal

# CONFIGURAÃ‡Ã•ES DO USUÃRIO (obrigatÃ³rio)
LOGGED_USER=codigo_usuario
CLIENT_ID=codigo_cliente
CLIENT_SECRET=codigo_secreto_cliente
X_CART=hash_carrinho_usuario

# USUÃRIOS AUTORIZADOS (opcional - separados por vÃ­rgula)
USERS=codigo1,codigo2,codigo3

# COTEFACIL API - OAuth2PasswordBearer (obrigatÃ³rio)
COTEFACIL_BEARER_TOKEN=seu_bearer_token_cotefacil
COTEFACIL_API_URL=https://desafio.cotefacil.net

# CALLBACK API - Credenciais para sistema de callbacks (obrigatÃ³rio)
CALLBACK_API_USER=seu_usuario_callback@dominio.com.br
CALLBACK_API_PASSWORD=sua_senha_callback
CALLBACK_URL=https://desafio.cotefacil.net

# URLs DO SISTEMA (configuraÃ§Ã£o padrÃ£o)
PORTAL_URL=https://pedidoeletronico.servimed.com.br
BASE_URL=https://peapi.servimed.com.br
API_ENDPOINT=/api/carrinho/oculto
SITE_VERSION=4.0.27

# Redis/Celery (opcional - para NÃ­vel 2)
REDIS_URL=redis://localhost:6379/0
CELERY_BROKER_URL=redis://localhost:6379/0

# Logs (opcional)
LOG_LEVEL=INFO
LOG_FILE=data/logs/servimed_scraper.log
```

---

## ğŸ§ª SISTEMA DE TESTES AUTOMATIZADOS

### ğŸ“Š EstatÃ­sticas dos Testes
- **Status:** âœ… Testes automatizados implementados
- **Funcionais:** Testes bÃ¡sicos funcionando
- **Categorias:** Unit, Integration, Error Handling

### ğŸ”§ Executando Testes

#### ğŸ¯ MÃ©todo 1: Script Automatizado
```bash
# Executar script interativo
python run_tests.py

# OpÃ§Ãµes disponÃ­veis:
# 1. Executar todos os testes
# 2. Executar teste especÃ­fico
# 3. Instalar dependÃªncias de teste
# 4. Verificar estrutura de testes
# 5. Sair
```

#### ğŸ¯ MÃ©todo 2: Pytest Direto
```bash
# Todos os testes
python -m pytest tests/ -v

# Testes especÃ­ficos
python -m pytest tests/test_basic_functionality.py -v
python -m pytest tests/test_config.py -v
python -m pytest tests/test_main.py -v

# Testes por categoria
python -m pytest tests/ -k "test_basic" -v
python -m pytest tests/ -k "integration" -v

# Com cobertura de cÃ³digo
python -m pytest tests/ --cov=src --cov=main --cov-report=html

# Apenas verificar estrutura
python -m pytest tests/ --collect-only
```

#### ğŸ¯ MÃ©todo 3: Testes EspecÃ­ficos
```bash
# Teste de funcionalidade bÃ¡sica (sempre funciona)
python -m pytest tests/test_basic_functionality.py::TestBasicFunctionality::test_python_environment -v

# Testes de configuraÃ§Ã£o de paths
python -m pytest tests/test_config.py::TestConfigPaths -v

# Testes do ScrapyWrapper
python -m pytest tests/test_scrapy_wrapper.py::TestScrapyWrapper::test_setup_logging -v
```

### ğŸ“‹ Tipos de Testes Implementados

#### ğŸ”§ **Testes BÃ¡sicos** âœ…
- âœ… Ambiente Python funcional
- âœ… Estrutura do projeto
- âœ… ImportaÃ§Ãµes de mÃ³dulos
- âœ… CriaÃ§Ã£o de objetos principais
- âœ… OperaÃ§Ãµes de arquivo
- âœ… DependÃªncias instaladas

#### ğŸ”— **Testes de ConfiguraÃ§Ã£o** âœ…
- âœ… Paths do projeto
- âœ… DiretÃ³rios de trabalho
- âœ… Arquivos de configuraÃ§Ã£o
- âœ… VariÃ¡veis de ambiente
- âœ… IntegraÃ§Ã£o de configuraÃ§Ãµes

#### ğŸš€ **Testes do Sistema**
- ScrapyWrapper functionality
- Sistema de filas Celery
- Cliente de pedidos
- IntegraÃ§Ã£o end-to-end
- Tratamento de erros

---

## ğŸ› ï¸ GUIA DE DESENVOLVIMENTO

### ğŸ” Debug e Logs
```bash
# Logs detalhados do Scrapy
python main.py --nivel 1 --filtro "test" --max-pages 1

# Logs do Celery (NÃ­vel 2)
python -m celery -A src.nivel2.celery_app worker --loglevel=debug

# Verificar logs em arquivo
# Windows: type data\logs\servimed_scraper.log
# Linux/Mac: cat data/logs/servimed_scraper.log
```

### ğŸ”§ Estrutura do CÃ³digo

#### ğŸ“„ main.py - Ponto de Entrada
```python
# FunÃ§Ãµes principais:
def executar_nivel_1(args)  # ExecuÃ§Ã£o direta
def executar_nivel_2(args)  # Sistema de filas  
def main()                  # Parser de argumentos e orquestraÃ§Ã£o
```

#### ğŸ“„ src/scrapy_wrapper.py - Wrapper Scrapy
```python
class ScrapyServimedWrapper:
    def run_spider(filtro="", max_pages=1)    # Executar spider
    def get_results()                          # Obter resultados
    def run_spider_subprocess(...)             # ExecuÃ§Ã£o via subprocess
```

#### ğŸ“„ src/nivel2/queue_client.py - Cliente de Filas
```python
class TaskQueueClient:
    def enqueue_scraping_task(...)             # Enfileirar scraping
    def get_task_status(task_id)               # Status da tarefa
    def get_worker_status()                    # Status dos workers
```

#### ğŸ“„ src/pedido_queue_client.py - Cliente de Pedidos
```python
class PedidoQueueClient:
    def enqueue_pedido(...)                    # Enfileirar pedido
    def get_status(task_id)                    # Status do pedido
```

### ğŸ“Š Fluxo de Dados

#### NÃ­vel 1: Direto
```
main.py â†’ ScrapyWrapper â†’ Scrapy Spider â†’ JSON File â†’ Results
```

#### NÃ­vel 2: Filas
```
main.py â†’ TaskQueueClient â†’ Celery Task â†’ ScrapyWrapper â†’ Callback API
```

#### NÃ­vel 3: Pedidos
```
pedido_queue_client.py â†’ Celery Task â†’ Order Processing â†’ External API
```

---

## ğŸš¨ TROUBLESHOOTING

### âŒ Problemas Comuns

#### 1. **Redis nÃ£o estÃ¡ rodando**
```bash
# Erro: ConnectionError: Error 10061 connecting to localhost:6379
# SoluÃ§Ã£o: Iniciar Redis
docker run -d -p 6379:6379 redis:latest
```

#### 2. **Workers Celery nÃ£o ativos**
```bash
# Erro: No workers available
# SoluÃ§Ã£o: Iniciar worker
python -m celery -A src.nivel2.celery_app worker --loglevel=info
```

#### 3. **DependÃªncias faltando**
```bash
# Erro: ModuleNotFoundError
# SoluÃ§Ã£o: Instalar dependÃªncias
python -m pip install -r requirements.txt
```

#### 4. **Testes falhando**
```bash
# Verificar ambiente bÃ¡sico primeiro
python -m pytest tests/test_basic_functionality.py -v

# Se bÃ¡sicos passarem, problema Ã© nos mocks/imports especÃ­ficos
```

#### 5. **Arquivo .env nÃ£o configurado**
```bash
# Criar .env com variÃ¡veis necessÃ¡rias
# Ver seÃ§Ã£o "VariÃ¡veis de Ambiente" acima
```

### ğŸ”§ Debug Mode
```bash
# Scrapy com debug
python -c "
import sys
sys.path.insert(0, 'src')
from scrapy_wrapper import ScrapyServimedWrapper
wrapper = ScrapyServimedWrapper()
print('Wrapper criado com sucesso!')
"

# Celery com debug
python -c "
import sys
sys.path.insert(0, 'src')
from nivel2.queue_client import TaskQueueClient
client = TaskQueueClient()
print('Cliente criado com sucesso!')
"
```

---

## ğŸ“ˆ MÃ‰TRICAS E PERFORMANCE

### ğŸ” Monitoramento
```bash
# Status do sistema
python main.py --nivel 2 --worker-status

# Logs em tempo real
# Windows PowerShell: Get-Content data\logs\servimed_scraper.log -Wait
# Linux/Mac: tail -f data/logs/servimed_scraper.log
```

---

## ğŸ”§ COMANDOS ESSENCIAIS

### ğŸ“Š Para Desenvolvimento
```bash
# Executar scraping bÃ¡sico
python main.py --nivel 1 --max-pages 1

# Executar testes bÃ¡sicos
python -m pytest tests/test_basic_functionality.py -v

# Ver estrutura de testes
python -m pytest tests/ --collect-only

# Executar teste especÃ­fico
python -m pytest tests/test_config.py::TestConfigPaths::test_project_root_path -v
```

### ğŸš¨ Para Troubleshooting
```bash
# Verificar se arquivos existem
# Windows: dir main.py / dir src\scrapy_wrapper.py / dir tests\test_basic_functionality.py
# Linux/Mac: ls main.py / ls src/scrapy_wrapper.py / ls tests/test_basic_functionality.py

# Testar imports manualmente
python -c "import main; print('main.py OK')"
python -c "import sys; sys.path.insert(0, 'src'); import scrapy_wrapper; print('scrapy_wrapper OK')"

# Ver conteÃºdo de arquivos importantes
# Windows: type pytest.ini / type requirements.txt
# Linux/Mac: cat pytest.ini / cat requirements.txt
```

### ğŸ“ˆ Para ProduÃ§Ã£o (NÃ­veis 2 e 3)
```bash
# NÃ­vel 2 - Setup (requer Redis)
# 1. Instalar Redis: docker run -d -p 6379:6379 redis:latest
# 2. Iniciar worker: python -m celery -A src.nivel2.celery_app worker --loglevel=info
# 3. Enfileirar: python main.py --nivel 2 --enqueue --usuario "test@example.com" --senha "password"

# NÃ­vel 3 - Pedidos
python src/pedido_queue_client.py test
python main.py --nivel 3
```

---

## âœ… CHECKLIST DE VERIFICAÃ‡ÃƒO

### ğŸ§ª Checklist de Funcionalidades

#### âœ… Core Functionalities
- [x] **Scraping BÃ¡sico** - NÃ­vel 1 funcionando
- [x] **Testes Automatizados** - Testes bÃ¡sicos implementados
- [x] **Estrutura Modular** - Imports e mÃ³dulos OK
- [x] **ConfiguraÃ§Ãµes** - Paths e settings funcionando
- [x] **Logging** - Sistema de logs ativo
- [x] **Error Handling** - Tratamento de erros implementado

#### ğŸ”„ Advanced Features (configuraÃ§Ã£o adicional necessÃ¡ria)
- [ ] **Sistema de Filas** - Requer Redis + Celery workers
- [ ] **API de Pedidos** - Requer configuraÃ§Ã£o de credenciais
- [ ] **Callbacks HTTP** - Requer endpoints externos
- [ ] **Monitoramento** - Requer dashboard setup

#### ğŸ§ª Quality Assurance
- [x] **Unit Tests** - Testes unitÃ¡rios funcionando
- [x] **Integration Tests** - Testes bÃ¡sicos de integraÃ§Ã£o
- [x] **Environment Tests** - ValidaÃ§Ã£o de ambiente
- [x] **Configuration Tests** - Testes de configuraÃ§Ã£o
- [x] **Documentation** - DocumentaÃ§Ã£o completa


---


## ğŸ“ SUPORTE RÃPIDO

### âŒ Se algo nÃ£o funcionar:
1. **Execute primeiro:** `python -m pytest tests/test_basic_functionality.py::TestBasicFunctionality::test_python_environment -v`
2. **Se falhar:** Problema no ambiente Python
3. **Se passar:** Execute teste completo: `python -m pytest tests/test_basic_functionality.py -v`
4. **Para debug:** Use comandos da seÃ§Ã£o troubleshooting

### âœ… Se tudo funcionar:
1. **Continue** com scraping bÃ¡sico: `python main.py --nivel 1 --max-pages 1`
2. **Explore** filtros: `python main.py --nivel 1 --filtro "seu_termo" --max-pages 2`
3. **Avance** para nÃ­veis superiores conforme necessidade

### ğŸ“š Recursos Adicionais
- **Scrapy Docs:** https://docs.scrapy.org/
- **Celery Docs:** https://docs.celeryproject.org/
- **Pytest Docs:** https://docs.pytest.org/

