# ğŸ“– DOCUMENTAÃ‡ÃƒO COMPLETA DO PROJETO SERVIMED SCRAPER

## ğŸ—ï¸ VISÃƒO GERAL

O **Servimed Scraper** Ã© um sistema completo de web scraping desenvolvido em Python para extrair informaÃ§Ãµes de produtos do site Servimed. O projeto implementa uma arquitetura em 3 nÃ­veis de complexidade, sempre utilizando o framework **Scrapy** como base para garantir robustez e escalabilidade.

### ğŸ¯ OBJETIVOS
- âœ… Extrair dados de produtos farmacÃªuticos do site Servimed
- âœ… Oferecer mÃºltiplas formas de execuÃ§Ã£o (sÃ­ncrona e assÃ­ncrona)
- âœ… Implementar sistema de filas para processamento em larga escala
- âœ… Fornecer API para integraÃ§Ã£o com sistemas externos
- âœ… Garantir qualidade atravÃ©s de testes automatizados

---

## ğŸ›ï¸ ARQUITETURA DO SISTEMA

### ğŸ“Š Estrutura de DiretÃ³rios
```
PROVA/
â”œâ”€â”€ ğŸ“„ main.py                      # Arquivo principal - ponto de entrada
â”œâ”€â”€ ğŸ“„ requirements.txt             # DependÃªncias do projeto
â”œâ”€â”€ ğŸ“„ .env                         # VariÃ¡veis de ambiente (configurar)
â”œâ”€â”€ ğŸ“„ pytest.ini                  # ConfiguraÃ§Ã£o dos testes
â”œâ”€â”€ ğŸ“„ pyproject.toml              # ConfiguraÃ§Ãµes avanÃ§adas
â”œâ”€â”€ ğŸ“„ run_tests.py                # Script para executar testes
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ“ src/                        # CÃ³digo fonte principal
â”‚   â”œâ”€â”€ ğŸ“„ scrapy_wrapper.py       # Wrapper principal do Scrapy
â”‚   â”œâ”€â”€ ğŸ“„ pedido_queue_client.py  # Cliente para pedidos (NÃ­vel 3)
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ config/                 # ConfiguraÃ§Ãµes do sistema
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ config.py           # ConfiguraÃ§Ãµes gerais
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ paths.py            # Caminhos e diretÃ³rios
â”‚   â”‚   â””â”€â”€ ğŸ“„ settings.py         # ConfiguraÃ§Ãµes especÃ­ficas
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ nivel2/                 # Sistema de filas (NÃ­vel 2)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ queue_client.py     # Cliente de filas
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ tasks.py            # Tarefas Celery
â”‚   â”‚   â””â”€â”€ ğŸ“„ celery_app.py       # ConfiguraÃ§Ã£o Celery
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ scrapy_servimed/        # Projeto Scrapy
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ settings.py         # ConfiguraÃ§Ãµes Scrapy
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ pipelines.py        # Pipelines de processamento
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ items.py            # DefiniÃ§Ã£o de items
â”‚   â”‚   â””â”€â”€ ğŸ“ spiders/            # Spiders de scraping
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚       â””â”€â”€ ğŸ“„ servimed_spider.py  # Spider principal
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ servimed_scraper/       # Sistema de processamento
â”‚       â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”œâ”€â”€ ğŸ“„ celery_app.py       # App Celery principal
â”‚       â””â”€â”€ ğŸ“„ tasks.py            # Tarefas de processamento
â”‚
â”œâ”€â”€ ğŸ“ tests/                      # Testes automatizados (84 testes)
â”‚   â”œâ”€â”€ ğŸ“„ conftest.py             # ConfiguraÃ§Ãµes globais de teste
â”‚   â”œâ”€â”€ ğŸ“„ test_basic_functionality.py  # Testes bÃ¡sicos (23 testes âœ…)
â”‚   â”œâ”€â”€ ğŸ“„ test_config.py          # Testes de configuraÃ§Ã£o (12 testes)
â”‚   â”œâ”€â”€ ğŸ“„ test_main.py            # Testes do main.py (10 testes)
â”‚   â”œâ”€â”€ ğŸ“„ test_scrapy_wrapper.py  # Testes do wrapper (12 testes)
â”‚   â”œâ”€â”€ ğŸ“„ test_spiders.py         # Testes dos spiders (14 testes)
â”‚   â”œâ”€â”€ ğŸ“„ test_integration.py     # Testes de integraÃ§Ã£o (13 testes)
â”‚   â”œâ”€â”€ ğŸ“ test_nivel2/
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_tasks.py       # Testes Celery (6 testes)
â”‚   â””â”€â”€ ğŸ“ test_nivel3/
â”‚       â””â”€â”€ ğŸ“„ test_pedido_queue_client.py  # Testes pedidos (14 testes)
â”‚
â”œâ”€â”€ ğŸ“ data/                       # Dados e resultados
â”‚   â”œâ”€â”€ ğŸ“„ servimed_produtos_scrapy.json   # Resultados Scrapy
â”‚   â””â”€â”€ ğŸ“„ logs/                   # Logs do sistema
â”‚
â””â”€â”€ ğŸ“ docs/                       # DocumentaÃ§Ã£o
    â”œâ”€â”€ ğŸ“„ PROJETO_ORGANIZADO.md   # OrganizaÃ§Ã£o do projeto
    â””â”€â”€ ğŸ“„ TESTS_SUMMARY.md        # Resumo dos testes
```

---

## ğŸš€ NÃVEIS DE EXECUÃ‡ÃƒO

### ğŸ¯ NÃVEL 1: EXECUÃ‡ÃƒO DIRETA (SÃNCRONA)
**DescriÃ§Ã£o:** ExecuÃ§Ã£o direta e imediata usando Scrapy  
**Uso:** Para testes, desenvolvimento e pequenos volumes  
**Framework:** Scrapy 2.13.3

#### ğŸ”§ Como Usar:
```bash
# ExecuÃ§Ã£o bÃ¡sica
C:/Python3.10_x64/Python310/python.exe main.py --nivel 1

# Com filtro de produtos
C:/Python3.10_x64/Python310/python.exe main.py --nivel 1 --filtro "paracetamol"

# Limitando pÃ¡ginas
C:/Python3.10_x64/Python310/python.exe main.py --nivel 1 --max-pages 5

# Combinando opÃ§Ãµes
C:/Python3.10_x64/Python310/python.exe main.py --nivel 1 --filtro "dipirona" --max-pages 3
```

#### ğŸ“Š SaÃ­da:
- Arquivo: `data/servimed_produtos_scrapy.json`
- Format: JSON com lista de produtos
- Logs: Console em tempo real

---

### ğŸ¯ NÃVEL 2: SISTEMA DE FILAS (ASSÃNCRONA)
**DescriÃ§Ã£o:** Processamento via filas usando Celery + Redis  
**Uso:** Para processamento em larga escala e produÃ§Ã£o  
**Framework:** Scrapy + Celery + Redis

#### ğŸ”§ PrÃ©-requisitos:
```bash
# 1. Instalar e iniciar Redis
# Windows: Baixar Redis ou usar Docker
docker run -d -p 6379:6379 redis:latest

# 2. Iniciar worker Celery
C:/Python3.10_x64/Python310/python.exe -m celery -A src.nivel2.celery_app worker --loglevel=info
```

#### ğŸ”§ Como Usar:
```bash
# Verificar status dos workers
C:/Python3.10_x64/Python310/python.exe main.py --nivel 2 --worker-status

# Enfileirar nova tarefa
C:/Python3.10_x64/Python310/python.exe main.py --nivel 2 --enqueue --usuario "user@example.com" --senha "password"

# Verificar status de tarefa
C:/Python3.10_x64/Python310/python.exe main.py --nivel 2 --status "task-id-aqui"

# Com filtro e callback customizado
C:/Python3.10_x64/Python310/python.exe main.py --nivel 2 --enqueue --filtro "antibiotico" --callback-url "https://api.exemplo.com/webhook"
```

#### ğŸ“Š SaÃ­da:
- Task ID para acompanhamento
- Callback HTTP para notificaÃ§Ã£o
- Resultados via API status

---

### ğŸ¯ NÃVEL 3: SISTEMA DE PEDIDOS
**DescriÃ§Ã£o:** Sistema completo de processamento de pedidos  
**Uso:** Para automaÃ§Ã£o de pedidos farmacÃªuticos  
**Framework:** Scrapy + Celery + Sistema de Pedidos

#### ğŸ”§ Como Usar:
```bash
# Teste do sistema
C:/Python3.10_x64/Python310/python.exe src/pedido_queue_client.py test

# Enfileirar pedido
C:/Python3.10_x64/Python310/python.exe src/pedido_queue_client.py enqueue "PEDIDO123" "444212" "2" "1234567890123"

# Verificar status do pedido
C:/Python3.10_x64/Python310/python.exe src/pedido_queue_client.py status "task-id"

# Via main.py (orientaÃ§Ãµes)
C:/Python3.10_x64/Python310/python.exe main.py --nivel 3
```

#### ğŸ“Š SaÃ­da:
- Task ID do pedido
- Status detalhado via API
- IntegraÃ§Ã£o com sistemas externos

---

## âš™ï¸ CONFIGURAÃ‡ÃƒO DO AMBIENTE

### ğŸ“‹ DependÃªncias Principais
```txt
# Framework de Scraping
scrapy>=2.13.3

# Sistema de Filas
celery>=5.3.4
redis>=5.0.1

# Testes Automatizados
pytest>=7.4.0
pytest-asyncio>=0.21.0
pytest-cov>=4.1.0
pytest-mock>=3.11.0

# UtilitÃ¡rios
requests>=2.31.0
python-dotenv>=1.0.0
lxml>=4.9.3
```

### ğŸ”§ InstalaÃ§Ã£o
```bash
# 1. Clonar/baixar o projeto
cd "C:\Users\6128347\OneDrive - Thomson Reuters Incorporated\Documents\Scrips\Tests\PROVA"

# 2. Configurar Python (jÃ¡ configurado)
# Python 3.10.0 em: C:/Python3.10_x64/Python310/python.exe

# 3. Instalar dependÃªncias
C:/Python3.10_x64/Python310/python.exe -m pip install -r requirements.txt

# 4. Configurar variÃ¡veis de ambiente (.env)
# Copiar .env.example para .env e ajustar valores
```

### ğŸ” VariÃ¡veis de Ambiente (.env)
```env
# API de Callback
CALLBACK_API_USER=seu_usuario@example.com
CALLBACK_API_PASSWORD=sua_senha_segura
CALLBACK_API_BASE_URL=https://desafio.cotefacil.net

# Redis/Celery
REDIS_URL=redis://localhost:6379/0
CELERY_BROKER_URL=redis://localhost:6379/0

# Scrapy
SERVIMED_BASE_URL=https://www.servimed.com.br
USER_AGENT=ServimedScraper/1.0

# Logs
LOG_LEVEL=INFO
LOG_FILE=data/logs/servimed_scraper.log
```

---

## ğŸ§ª SISTEMA DE TESTES AUTOMATIZADOS

### ğŸ“Š EstatÃ­sticas dos Testes
- **Total:** 84 testes automatizados
- **Funcionais:** 23 testes bÃ¡sicos âœ… (100% sucesso)
- **ConfiguraÃ§Ã£o:** 12 testes (91% sucesso)
- **Categorias:** Unit, Integration, Error Handling

### ğŸ”§ Executando Testes

#### ğŸ¯ MÃ©todo 1: Script Automatizado
```bash
# Executar script interativo
C:/Python3.10_x64/Python310/python.exe run_tests.py

# OpÃ§Ãµes disponÃ­veis:
# 1. Executar todos os testes
# 2. Executar teste especÃ­fico
# 3. Instalar dependÃªncias de teste
# 4. Verificar estrutura de testes
# 5. Sair
```

#### ğŸ¯ MÃ©todo 2: Pytest Direto
```bash
# Todos os testes
C:/Python3.10_x64/Python310/python.exe -m pytest tests/ -v

# Testes especÃ­ficos
C:/Python3.10_x64/Python310/python.exe -m pytest tests/test_basic_functionality.py -v
C:/Python3.10_x64/Python310/python.exe -m pytest tests/test_config.py -v
C:/Python3.10_x64/Python310/python.exe -m pytest tests/test_main.py -v

# Testes por categoria
C:/Python3.10_x64/Python310/python.exe -m pytest tests/ -k "test_basic" -v
C:/Python3.10_x64/Python310/python.exe -m pytest tests/ -k "integration" -v

# Com cobertura de cÃ³digo
C:/Python3.10_x64/Python310/python.exe -m pytest tests/ --cov=src --cov=main --cov-report=html

# Apenas verificar estrutura
C:/Python3.10_x64/Python310/python.exe -m pytest tests/ --collect-only
```

#### ğŸ¯ MÃ©todo 3: Testes EspecÃ­ficos
```bash
# Teste de funcionalidade bÃ¡sica (sempre funciona)
C:/Python3.10_x64/Python310/python.exe -m pytest tests/test_basic_functionality.py::TestBasicFunctionality::test_python_environment -v

# Testes de configuraÃ§Ã£o de paths
C:/Python3.10_x64/Python310/python.exe -m pytest tests/test_config.py::TestConfigPaths -v

# Testes do ScrapyWrapper
C:/Python3.10_x64/Python310/python.exe -m pytest tests/test_scrapy_wrapper.py::TestScrapyWrapper::test_setup_logging -v
```

### ğŸ“‹ Tipos de Testes Implementados

#### ğŸ”§ **Testes BÃ¡sicos** (23 testes - 100% âœ…)
- âœ… Ambiente Python funcional
- âœ… Estrutura do projeto
- âœ… ImportaÃ§Ãµes de mÃ³dulos
- âœ… CriaÃ§Ã£o de objetos principais
- âœ… OperaÃ§Ãµes de arquivo
- âœ… DependÃªncias instaladas

#### ğŸ”— **Testes de ConfiguraÃ§Ã£o** (12 testes - 91% âœ…)
- âœ… Paths do projeto
- âœ… DiretÃ³rios de trabalho
- âœ… Arquivos de configuraÃ§Ã£o
- âœ… VariÃ¡veis de ambiente
- âœ… IntegraÃ§Ã£o de configuraÃ§Ãµes

#### ğŸš€ **Testes do Sistema** (49 testes variados)
- ScrapyWrapper functionality
- Sistema de filas Celery
- Cliente de pedidos
- IntegraÃ§Ã£o end-to-end
- Tratamento de erros

---

## ğŸ› ï¸ GUIA DE DESENVOLVIMENTO

### ğŸ” Debug e Logs
```bash
# Logs detalhados do Scrapy
C:/Python3.10_x64/Python310/python.exe main.py --nivel 1 --filtro "test" --max-pages 1

# Logs do Celery (NÃ­vel 2)
C:/Python3.10_x64/Python310/python.exe -m celery -A src.nivel2.celery_app worker --loglevel=debug

# Verificar logs em arquivo
type data\logs\servimed_scraper.log
```

### ğŸ”§ Estrutura do CÃ³digo

#### ğŸ“„ main.py - Ponto de Entrada
```python
# FunÃ§Ãµes principais:
def executar_nivel_1(args)  # ExecuÃ§Ã£o direta
def executar_nivel_2(args)  # Sistema de filas  
def main()                  # Parser de argumentos e orquestraÃ§Ã£o
```

#### ğŸ“„ src/scrapy_wrapper.py - Wrapper Scrapy
```python
class ScrapyServimedWrapper:
    def run_spider(filtro="", max_pages=1)    # Executar spider
    def get_results()                          # Obter resultados
    def run_spider_subprocess(...)             # ExecuÃ§Ã£o via subprocess
```

#### ğŸ“„ src/nivel2/queue_client.py - Cliente de Filas
```python
class TaskQueueClient:
    def enqueue_scraping_task(...)             # Enfileirar scraping
    def get_task_status(task_id)               # Status da tarefa
    def get_worker_status()                    # Status dos workers
```

#### ğŸ“„ src/pedido_queue_client.py - Cliente de Pedidos
```python
class PedidoQueueClient:
    def enqueue_pedido(...)                    # Enfileirar pedido
    def get_status(task_id)                    # Status do pedido
```

### ğŸ“Š Fluxo de Dados

#### NÃ­vel 1: Direto
```
main.py â†’ ScrapyWrapper â†’ Scrapy Spider â†’ JSON File â†’ Results
```

#### NÃ­vel 2: Filas
```
main.py â†’ TaskQueueClient â†’ Celery Task â†’ ScrapyWrapper â†’ Callback API
```

#### NÃ­vel 3: Pedidos
```
pedido_queue_client.py â†’ Celery Task â†’ Order Processing â†’ External API
```

---

## ğŸš¨ TROUBLESHOOTING

### âŒ Problemas Comuns

#### 1. **Redis nÃ£o estÃ¡ rodando**
```bash
# Erro: ConnectionError: Error 10061 connecting to localhost:6379
# SoluÃ§Ã£o: Iniciar Redis
docker run -d -p 6379:6379 redis:latest
```

#### 2. **Workers Celery nÃ£o ativos**
```bash
# Erro: No workers available
# SoluÃ§Ã£o: Iniciar worker
C:/Python3.10_x64/Python310/python.exe -m celery -A src.nivel2.celery_app worker --loglevel=info
```

#### 3. **DependÃªncias faltando**
```bash
# Erro: ModuleNotFoundError
# SoluÃ§Ã£o: Instalar dependÃªncias
C:/Python3.10_x64/Python310/python.exe -m pip install -r requirements.txt
```

#### 4. **Testes falhando**
```bash
# Verificar ambiente bÃ¡sico primeiro
C:/Python3.10_x64/Python310/python.exe -m pytest tests/test_basic_functionality.py -v

# Se bÃ¡sicos passarem, problema Ã© nos mocks/imports especÃ­ficos
```

#### 5. **Arquivo .env nÃ£o configurado**
```bash
# Criar .env com variÃ¡veis necessÃ¡rias
# Ver seÃ§Ã£o "VariÃ¡veis de Ambiente" acima
```

### ğŸ”§ Debug Mode
```bash
# Scrapy com debug
C:/Python3.10_x64/Python310/python.exe -c "
import sys
sys.path.insert(0, 'src')
from scrapy_wrapper import ScrapyServimedWrapper
wrapper = ScrapyServimedWrapper()
print('Wrapper criado com sucesso!')
"

# Celery com debug
C:/Python3.10_x64/Python310/python.exe -c "
import sys
sys.path.insert(0, 'src')
from nivel2.queue_client import TaskQueueClient
client = TaskQueueClient()
print('Cliente criado com sucesso!')
"
```

---

## ğŸ“ˆ MÃ‰TRICAS E PERFORMANCE

### ğŸ¯ Benchmarks TÃ­picos
- **NÃ­vel 1:** ~1-5 segundos por pÃ¡gina
- **NÃ­vel 2:** Processamento paralelo, mÃºltiplos workers
- **NÃ­vel 3:** IntegraÃ§Ã£o completa com sistemas externos

### ğŸ“Š Limites Recomendados
- **max_pages:** AtÃ© 10 pÃ¡ginas para testes
- **Workers:** 2-4 workers simultÃ¢neos
- **Rate Limiting:** Respeitado automaticamente pelo Scrapy

### ğŸ” Monitoramento
```bash
# Status do sistema
C:/Python3.10_x64/Python310/python.exe main.py --nivel 2 --worker-status

# Logs em tempo real
tail -f data/logs/servimed_scraper.log  # Linux/Mac
Get-Content data\logs\servimed_scraper.log -Wait  # Windows PowerShell
```

---

## ğŸ”® PRÃ“XIMOS PASSOS

### ğŸš€ Melhorias Planejadas
1. **Dashboard Web** para monitoramento
2. **API REST** para integraÃ§Ã£o externa
3. **Docker Containers** para deploy
4. **CI/CD Pipeline** para automaÃ§Ã£o
5. **Banco de Dados** para persistÃªncia
6. **Rate Limiting** mais sofisticado

### ğŸ§ª ExpansÃ£o de Testes
1. **Testes de Performance** 
2. **Testes de Carga**
3. **Testes E2E** completos
4. **Cobertura 100%** de cÃ³digo

### ğŸ”§ OtimizaÃ§Ãµes
1. **Cache inteligente** de resultados
2. **Retry automÃ¡tico** em falhas
3. **Balanceamento** de carga
4. **Monitoramento** avanÃ§ado

---

## ğŸ‘¥ SUPORTE E CONTATO

### ğŸ“ Para Suporte
- **Testes:** Execute `test_basic_functionality.py` primeiro
- **Logs:** Verificar `data/logs/` para detalhes
- **Debug:** Usar comandos de debug acima

### ğŸ“š Recursos Adicionais
- **Scrapy Docs:** https://docs.scrapy.org/
- **Celery Docs:** https://docs.celeryproject.org/
- **Pytest Docs:** https://docs.pytest.org/

---

## ğŸŠ CONCLUSÃƒO

O **Servimed Scraper** Ã© um sistema robusto e escalÃ¡vel que oferece:

âœ… **3 nÃ­veis de complexidade** para diferentes necessidades  
âœ… **Framework Scrapy** em todos os nÃ­veis para consistÃªncia  
âœ… **84 testes automatizados** para garantir qualidade  
âœ… **DocumentaÃ§Ã£o completa** para facilitar uso e manutenÃ§Ã£o  
âœ… **Arquitetura modular** para fÃ¡cil extensÃ£o  
âœ… **Sistema de filas** para alta performance  
âœ… **IntegraÃ§Ã£o externa** via APIs  

**Ready for production! ğŸš€**

---

*Ãšltima atualizaÃ§Ã£o: 13 de Agosto de 2025*  
*VersÃ£o do Sistema: 2.0.0*  
*Framework: Scrapy 2.13.3*
